{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Duration', 'Source Port', 'Destination Port', 'Protocol', 'Flags',\n",
      "       'Packets', 'Bytes', 'Mean Payload Size', 'Std Payload Size',\n",
      "       'Min Payload Size', 'Max Payload Size', 'Mean Entropy', 'Min Entropy',\n",
      "       'Max Entropy', 'Mean Inter-Packet Interval',\n",
      "       'Min Inter-Packet Interval', 'Max Inter-Packet Interval',\n",
      "       'Bytes per Packet', 'Packets per Second', 'Bytes per Second',\n",
      "       'Destination Common Port Usage', 'Flags Count', 'SYN Count',\n",
      "       'ACK Count', 'FIN Count', 'Is HTTP', 'Is Internal IP', 'Direction',\n",
      "       'Short Duration', 'Single Packet'],\n",
      "      dtype='object')\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Index(['Duration', 'Source Port', 'Destination Port', 'Protocol', 'Flags',\n",
      "       'Packets', 'Bytes', 'Mean Payload Size', 'Std Payload Size',\n",
      "       'Min Payload Size', 'Max Payload Size', 'Mean Entropy', 'Min Entropy',\n",
      "       'Max Entropy', 'Mean Inter-Packet Interval',\n",
      "       'Min Inter-Packet Interval', 'Max Inter-Packet Interval',\n",
      "       'Bytes per Packet', 'Packets per Second', 'Bytes per Second',\n",
      "       'Destination Common Port Usage', 'Flags Count', 'SYN Count',\n",
      "       'ACK Count', 'FIN Count', 'Is HTTP', 'Is Internal IP', 'Direction',\n",
      "       'Short Duration', 'Single Packet'],\n",
      "      dtype='object')\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Index(['Duration', 'Source Port', 'Destination Port', 'Protocol', 'Flags',\n",
      "       'Packets', 'Bytes', 'Mean Payload Size', 'Std Payload Size',\n",
      "       'Min Payload Size', 'Max Payload Size', 'Mean Entropy', 'Min Entropy',\n",
      "       'Max Entropy', 'Mean Inter-Packet Interval',\n",
      "       'Min Inter-Packet Interval', 'Max Inter-Packet Interval',\n",
      "       'Bytes per Packet', 'Packets per Second', 'Bytes per Second',\n",
      "       'Destination Common Port Usage', 'Flags Count', 'SYN Count',\n",
      "       'ACK Count', 'FIN Count', 'Is HTTP', 'Is Internal IP', 'Direction',\n",
      "       'Short Duration', 'Single Packet', 'Label'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 31 features, but StandardScaler is expecting 30 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(folder):\n\u001b[1;32m     83\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, file_name)\n\u001b[0;32m---> 84\u001b[0m         \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Convert results to a DataFrame\u001b[39;00m\n\u001b[1;32m     87\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "Cell \u001b[0;32mIn[58], line 50\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     47\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirection\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m direction_encoder\u001b[38;5;241m.\u001b[39mtransform(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirection\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     49\u001b[0m columns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m---> 50\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df, columns\u001b[38;5;241m=\u001b[39mcolumns)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Reshape for LSTM: (samples, time steps, features)\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/c2-detection-TvTzD0kY-py3.10/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/c2-detection-TvTzD0kY-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/c2-detection-TvTzD0kY-py3.10/lib/python3.10/site-packages/sklearn/base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/c2-detection-TvTzD0kY-py3.10/lib/python3.10/site-packages/sklearn/base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 31 features, but StandardScaler is expecting 30 features as input."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load saved LSTM model (ensure the model was saved using model.save(\"LSTM_model.h5\"))\n",
    "model = tf.keras.models.load_model(\"../../models/dridex/LSTM.h5\")\n",
    "\n",
    "# Load encoders and scaler\n",
    "with open(\"../../variables/dridex/Protocol_Encoder.pkl\", \"rb\") as f:\n",
    "    protocol_encoder = pickle.load(f)\n",
    "\n",
    "with open(\"../../variables/dridex/Flags_Encoder.pkl\", \"rb\") as f:\n",
    "    flags_encoder = pickle.load(f)\n",
    "\n",
    "with open(\"../../variables/dridex/Direction_Encoder.pkl\", \"rb\") as f:\n",
    "    direction_encoder = pickle.load(f)\n",
    "\n",
    "with open(\"../../variables/dridex/scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# Paths to data folders\n",
    "folders = [\n",
    "    \"../../data/collected/normal_machines\",\n",
    "    \"../../data/collected/infected_machines\",\n",
    "]\n",
    "\n",
    "# Prediction threshold\n",
    "THRESHOLD = 0.5  # Since sigmoid activation is used\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "def process_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.loc[((df[\"Flags\"] == \"SYN,RST\") | (df[\"Flags\"] == \"RST,ACK\")), \"Flags\"] = (\n",
    "        \"RST\"\n",
    "    )\n",
    "    df.loc[((df[\"Protocol\"] == \"DATA-TEXT-LINES\")|(df[\"Protocol\"] == \"XML\")), \"Protocol\"] = \"MEDIA\"\n",
    "    print(df.columns)\n",
    "\n",
    "    df[\"Protocol\"] = protocol_encoder.transform(df[\"Protocol\"])\n",
    "    df[\"Flags\"] = flags_encoder.transform(df[\"Flags\"])\n",
    "    df[\"Direction\"] = direction_encoder.transform(df[\"Direction\"])\n",
    "\n",
    "    columns = df.columns\n",
    "    df = scaler.transform(df.to_numpy())\n",
    "    df = pd.DataFrame(df, columns=columns)\n",
    "\n",
    "    # Reshape for LSTM: (samples, time steps, features)\n",
    "    X_lstm = np.expand_dims(df.values, axis=1)  # Fix input shape\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_lstm)\n",
    "\n",
    "    # Convert probabilities to class labels\n",
    "    predicted_labels = (predictions > THRESHOLD).astype(int)  # 1 = Dridex, 0 = Benign\n",
    "\n",
    "    # Count\n",
    "    benign_count = np.sum(predicted_labels == 0)\n",
    "    dridex_count = np.sum(predicted_labels == 1)\n",
    "\n",
    "    # Determine file classification\n",
    "    file_status = \"Malicious\" if dridex_count > 0 else \"Benign\"\n",
    "\n",
    "    # Store results in a list\n",
    "    results.append(\n",
    "        {\n",
    "            \"File\": file_path,\n",
    "            \"Benign_Count\": benign_count,\n",
    "            \"Dridex_Count\": dridex_count,\n",
    "            \"Final_Classification\": file_status,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Process all files in both folders\n",
    "for folder in folders:\n",
    "    for file_name in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        process_file(file_path)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV (optional)\n",
    "results_df.to_csv(\"classification_results.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Benign_Count</th>\n",
       "      <th>Dridex_Count</th>\n",
       "      <th>Final_Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../data/collected/normal_machines/benign_3.csv</td>\n",
       "      <td>528</td>\n",
       "      <td>133</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../data/collected/normal_machines/benign_2.csv</td>\n",
       "      <td>517</td>\n",
       "      <td>107</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../data/collected/normal_machines/benign_5.csv</td>\n",
       "      <td>480</td>\n",
       "      <td>114</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../data/collected/normal_machines/benign_4.csv</td>\n",
       "      <td>417</td>\n",
       "      <td>104</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../data/collected/normal_machines/benign_1.csv</td>\n",
       "      <td>492</td>\n",
       "      <td>139</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>../../data/collected/infected_machines/infecte...</td>\n",
       "      <td>485</td>\n",
       "      <td>141</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>../../data/collected/infected_machines/infecte...</td>\n",
       "      <td>430</td>\n",
       "      <td>118</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>../../data/collected/infected_machines/infecte...</td>\n",
       "      <td>533</td>\n",
       "      <td>161</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>../../data/collected/infected_machines/infecte...</td>\n",
       "      <td>547</td>\n",
       "      <td>133</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>../../data/collected/infected_machines/infecte...</td>\n",
       "      <td>525</td>\n",
       "      <td>137</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                File  Benign_Count  \\\n",
       "0  ../../data/collected/normal_machines/benign_3.csv           528   \n",
       "1  ../../data/collected/normal_machines/benign_2.csv           517   \n",
       "2  ../../data/collected/normal_machines/benign_5.csv           480   \n",
       "3  ../../data/collected/normal_machines/benign_4.csv           417   \n",
       "4  ../../data/collected/normal_machines/benign_1.csv           492   \n",
       "5  ../../data/collected/infected_machines/infecte...           485   \n",
       "6  ../../data/collected/infected_machines/infecte...           430   \n",
       "7  ../../data/collected/infected_machines/infecte...           533   \n",
       "8  ../../data/collected/infected_machines/infecte...           547   \n",
       "9  ../../data/collected/infected_machines/infecte...           525   \n",
       "\n",
       "   Dridex_Count Final_Classification  \n",
       "0           133            Malicious  \n",
       "1           107            Malicious  \n",
       "2           114            Malicious  \n",
       "3           104            Malicious  \n",
       "4           139            Malicious  \n",
       "5           141            Malicious  \n",
       "6           118            Malicious  \n",
       "7           161            Malicious  \n",
       "8           133            Malicious  \n",
       "9           137            Malicious  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                File  Benign_Count  \\\n",
      "0  ../../data/collected/normal_machines/benign_3.csv           606   \n",
      "1  ../../data/collected/normal_machines/benign_2.csv           572   \n",
      "2  ../../data/collected/normal_machines/benign_5.csv           544   \n",
      "3  ../../data/collected/normal_machines/benign_4.csv           481   \n",
      "4  ../../data/collected/normal_machines/benign_1.csv           574   \n",
      "5  ../../data/collected/infected_machines/infecte...           457   \n",
      "6  ../../data/collected/infected_machines/infecte...           384   \n",
      "7  ../../data/collected/infected_machines/infecte...           486   \n",
      "8  ../../data/collected/infected_machines/infecte...           487   \n",
      "9  ../../data/collected/infected_machines/infecte...           474   \n",
      "\n",
      "   Dridex_Count Final_Classification  \n",
      "0            55            Malicious  \n",
      "1            52            Malicious  \n",
      "2            50            Malicious  \n",
      "3            40            Malicious  \n",
      "4            57            Malicious  \n",
      "5           169            Malicious  \n",
      "6           164            Malicious  \n",
      "7           208            Malicious  \n",
      "8           193            Malicious  \n",
      "9           188            Malicious  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "with open(\"../../models/dridex/logreg.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Load encoders and scaler\n",
    "with open(\"../../variables/dridex/Protocol_Encoder.pkl\", \"rb\") as f:\n",
    "    protocol_encoder = pickle.load(f)\n",
    "\n",
    "with open(\"../../variables/dridex/Flags_Encoder.pkl\", \"rb\") as f:\n",
    "    flags_encoder = pickle.load(f)\n",
    "\n",
    "with open(\"../../variables/dridex/Direction_Encoder.pkl\", \"rb\") as f:\n",
    "    direction_encoder = pickle.load(f)\n",
    "\n",
    "with open(\"../../variables/dridex/scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# Paths to data folders\n",
    "folders = [\n",
    "    \"../../data/collected/normal_machines\",\n",
    "    \"../../data/collected/infected_machines\",\n",
    "]\n",
    "\n",
    "# Prediction threshold\n",
    "THRESHOLD = 0.5  # Since sigmoid activation is used\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "def process_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.loc[((df[\"Flags\"] == \"SYN,RST\") | (df[\"Flags\"] == \"RST,ACK\")), \"Flags\"] = (\n",
    "        \"RST\"\n",
    "    )\n",
    "    df.loc[((df[\"Protocol\"] == \"DATA-TEXT-LINES\")|(df[\"Protocol\"] == \"XML\")), \"Protocol\"] = \"MEDIA\"\n",
    "\n",
    "    df[\"Protocol\"] = protocol_encoder.transform(df[\"Protocol\"])\n",
    "    df[\"Flags\"] = flags_encoder.transform(df[\"Flags\"])\n",
    "    df[\"Direction\"] = direction_encoder.transform(df[\"Direction\"])\n",
    "\n",
    "    columns = df.columns\n",
    "    df = scaler.transform(df.to_numpy())\n",
    "    df = pd.DataFrame(df, columns=columns)\n",
    "    # Make predictions\n",
    "    predicted_labels = model.predict(df)\n",
    "\n",
    "\n",
    "    benign_count = np.sum(predicted_labels == 0)\n",
    "    dridex_count = np.sum(predicted_labels == 1)\n",
    "\n",
    "    # Determine file classification\n",
    "    file_status = \"Malicious\" if dridex_count > 0 else \"Benign\"\n",
    "\n",
    "    # Store results in a list\n",
    "    results.append(\n",
    "        {\n",
    "            \"File\": file_path,\n",
    "            \"Benign_Count\": benign_count,\n",
    "            \"Dridex_Count\": dridex_count,\n",
    "            \"Final_Classification\": file_status,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Process all files in both folders\n",
    "for folder in folders:\n",
    "    for file_name in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        process_file(file_path)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV (optional)\n",
    "results_df.to_csv(\"classification_results.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Benign_Count</th>\n",
       "      <th>Dridex_Count</th>\n",
       "      <th>Final_Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../data/collected/normal_machines/benign_3.csv</td>\n",
       "      <td>606</td>\n",
       "      <td>55</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../data/collected/normal_machines/benign_2.csv</td>\n",
       "      <td>572</td>\n",
       "      <td>52</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../data/collected/normal_machines/benign_5.csv</td>\n",
       "      <td>544</td>\n",
       "      <td>50</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../data/collected/normal_machines/benign_4.csv</td>\n",
       "      <td>481</td>\n",
       "      <td>40</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../data/collected/normal_machines/benign_1.csv</td>\n",
       "      <td>574</td>\n",
       "      <td>57</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>../../data/collected/infected_machines/infecte...</td>\n",
       "      <td>457</td>\n",
       "      <td>169</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>../../data/collected/infected_machines/infecte...</td>\n",
       "      <td>384</td>\n",
       "      <td>164</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>../../data/collected/infected_machines/infecte...</td>\n",
       "      <td>486</td>\n",
       "      <td>208</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>../../data/collected/infected_machines/infecte...</td>\n",
       "      <td>487</td>\n",
       "      <td>193</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>../../data/collected/infected_machines/infecte...</td>\n",
       "      <td>474</td>\n",
       "      <td>188</td>\n",
       "      <td>Malicious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                File  Benign_Count  \\\n",
       "0  ../../data/collected/normal_machines/benign_3.csv           606   \n",
       "1  ../../data/collected/normal_machines/benign_2.csv           572   \n",
       "2  ../../data/collected/normal_machines/benign_5.csv           544   \n",
       "3  ../../data/collected/normal_machines/benign_4.csv           481   \n",
       "4  ../../data/collected/normal_machines/benign_1.csv           574   \n",
       "5  ../../data/collected/infected_machines/infecte...           457   \n",
       "6  ../../data/collected/infected_machines/infecte...           384   \n",
       "7  ../../data/collected/infected_machines/infecte...           486   \n",
       "8  ../../data/collected/infected_machines/infecte...           487   \n",
       "9  ../../data/collected/infected_machines/infecte...           474   \n",
       "\n",
       "   Dridex_Count Final_Classification  \n",
       "0            55            Malicious  \n",
       "1            52            Malicious  \n",
       "2            50            Malicious  \n",
       "3            40            Malicious  \n",
       "4            57            Malicious  \n",
       "5           169            Malicious  \n",
       "6           164            Malicious  \n",
       "7           208            Malicious  \n",
       "8           193            Malicious  \n",
       "9           188            Malicious  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 31 features, but StandardScaler is expecting 30 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(folder):\n\u001b[1;32m     30\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, file_name)\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 17\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirection\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m direction_encoder\u001b[38;5;241m.\u001b[39mtransform(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirection\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     16\u001b[0m columns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m---> 17\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df, columns\u001b[38;5;241m=\u001b[39mcolumns)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/c2-detection-TvTzD0kY-py3.10/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/c2-detection-TvTzD0kY-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/c2-detection-TvTzD0kY-py3.10/lib/python3.10/site-packages/sklearn/base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/c2-detection-TvTzD0kY-py3.10/lib/python3.10/site-packages/sklearn/base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 31 features, but StandardScaler is expecting 30 features as input."
     ]
    }
   ],
   "source": [
    "folders = [\n",
    "    \"../../data/collected/normal_machines\",\n",
    "]                         \n",
    "def process_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfc = df.copy()\n",
    "    df.loc[((df[\"Flags\"] == \"SYN,RST\") | (df[\"Flags\"] == \"RST,ACK\")), \"Flags\"] = \"RST\"\n",
    "    df.loc[\n",
    "        ((df[\"Protocol\"] == \"DATA-TEXT-LINES\") | (df[\"Protocol\"] == \"XML\")), \"Protocol\"\n",
    "    ] = \"MEDIA\"\n",
    "\n",
    "    df[\"Protocol\"] = protocol_encoder.transform(df[\"Protocol\"])\n",
    "    df[\"Flags\"] = flags_encoder.transform(df[\"Flags\"])\n",
    "    df[\"Direction\"] = direction_encoder.transform(df[\"Direction\"])\n",
    "\n",
    "    columns = df.columns\n",
    "    df = scaler.transform(df.to_numpy())\n",
    "    df = pd.DataFrame(df, columns=columns)\n",
    "    # Make predictions\n",
    "    predicted_labels = model.predict(df)\n",
    "    dfc[\"Label\"] = predicted_labels\n",
    "    dfc = dfc[dfc[\"Label\"] <= 0.5]\n",
    "    dfc.drop(\"Label\", axis=1)\n",
    "    dfc.reset_index()\n",
    "    dfc.to_csv(f\"{file_path}.csv\")\n",
    "\n",
    "\n",
    "# Process all files in both folders\n",
    "for folder in folders:\n",
    "    for file_name in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        process_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c2-detection-TvTzD0kY-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
